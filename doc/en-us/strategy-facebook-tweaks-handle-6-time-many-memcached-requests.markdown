## [Strategy: Facebook Tweaks to Handle 6 Time as Many Memcached Requests](/blog/2008/12/13/strategy-facebook-tweaks-to-handle-6-time-as-many-memcached.html)

<div class="journal-entry-tag journal-entry-tag-post-title"><span class="posted-on">![Date](/universal/images/transparent.png "Date")Saturday, December 13, 2008 at 5:19AM</span></div>

<div class="body">

Our latest strategy is taken from a [great post by Paul Saab of Facebook](http://www.facebook.com/note.php?note_id=39391378919), detailing how with changes Facebook has made to memcached they have:

> ...been able to scale memcached to handle 200,000 UDP requests per second with an average latency of 173 microseconds. The total throughput achieved is 300,000 UDP requests/s, but the latency at that request rate is too high to be useful in our system. This is an amazing increase from 50,000 UDP requests/s using the stock version of Linux and memcached.

To scale Facebook has hundreds of thousands of TCP connections open to their memcached processes. First, this is still amazing. It's not so long ago you could have never done this. Optimizing connection use was always a priority because the OS simply couldn't handle large numbers of connections or large numbers of threads or large numbers of CPUs. To get to this point is a big accomplishment. Still, at that scale there are problems that are often solved.  

Some of the problem Facebook faced and fixed:

*   **Per connection consumption of resources**. What works well at low number of inputs can totally kill a system as inputs grow. Memcached uses a per-connection buffer which adds up to a lot of memory that could be used to store data. Nothing wrong with this design choice, but Facebook made changes to use a per-thread shared connection buffer and reclaimed gigabytes of RAM on each server.*   **Kernel lock contention**. Facebook discovered under load there was lock contention when transmitting through a single UDP socket from multiple threads. Sockets are data structures too and they are subject to the usual lock contention issues. Facebook got around this issue by maintaining separate reply sockets in different threads so they would not contend with the receive sockets. They found another bottleneck in Linux’s “netdevice” layer that sits in-between IP and device drivers. They changed the dequeue algorithm to batch dequeues so more work was done when they had the CPU.*   **Application lock contention**. Nothing brings out lock issues like moving to more cores. Facebook found when they moved to 8 core machines a global lock protecting stats collection used 20-30% of CPU usage. In application that require little processing per request, as does memcached, this is not unexpected, but doing real work with your CPU is a better idea. So they collected stats on a per thread basis and then calculated a global view on demand.*   **Interrupt floods and starvation**. With so much traffic directed at a single server the hardware can flood the CPU(s) with interrupts and keep the CPU from doing "real" work. To get around this problem Facebook implements some complicated strategies to load balance IO across all the cores. As I am less clever I might try more network cards with a TCP Offload engine.  

    When you read Paul's article keep in mind all the incredible number of man hours that went into profiling the system, not just their application, but the entire software hardware stack. Then add in the research, planning, and trying different solutions to see if anything changed for the better. It's a lot of work. Notice using a nifty new parallel language or moving to a cloud wouldn't have made a bit difference. It's complete mastery of their system that made the difference.  

    A summary of potential strategies:*   **Profile everything**. Problems are always specific. The understanding of the problem must be specific. The fix must be specific.*   **Burn profiling into your regression tests**. Detect when and where performance tanks as a regular part of your build.*   **Use resources in proportion to what grows slowest**. This requires multiplexing, but at least your resource usage is more predictable and bounded.*   **Batch work**. When you have the CPU do all the work you possibly can in the quantum or the whole system grinds to a halt in processing overhead.*   **Do work and maintain resources per task**. Otherwise locking for shared resources takes more and more time when there's less and less time to do the work that needs to be done.*   **Change algorithms**. Sometimes you simply need to do things differently. Tweaking will only get you so far.  

    You can find their changes on [github](http://www.facebook.com/note_redirect.php?note_id=39391378919&h=ad011a56d1398e90d074fe748116d967&url=http%3A%2F%2Fgithub.com%2Ffbmarc%2Ffacebook-memcached), the hub that says "git."</div>